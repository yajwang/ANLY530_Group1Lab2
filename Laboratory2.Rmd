---
title: "ANLY 530 Group Assignment (Laboratory 2: Naive Bayes Classifiers)"
author:
- "Josseline Betio"
- "Akpany Benjamin Ebby"
- "Yajun Wang"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    highlight: zenburn
comment: Rmd Document with Markdown + Embedded R Code
---

---

#### Document as of `r Sys.Date()`

```{r setup, include=FALSE, purl=FALSE, eval=TRUE}

source('R/ANLY530_Laboratory2_Helper.R')

```

```{r echo=FALSE}

library(knitr)
library(kableExtra)
read_chunk('R/ANLY530_Laboratory2_Chunks.R')

```

Some helper functions do exists to support the code snippets below such as: ```fullFilePath```

---

```{r loadLibraries, message=FALSE, warning=FALSE}
```

## Collecting the data

Let us start by [Loading the Data Sheets]

```{r loadSheets, message=FALSE, warning=FALSE}
```

# Credit Data
-------------

# Part 1:

## Step 1: Exploring and Preparing the Data

```{r part1Step1, message=FALSE, warning=FALSE}
```

## Step 2: Training a Model on the Data

```{r part1Step2, message=FALSE, warning=FALSE}
```

## Step 3: Evaluating Model Performance

```{r part1Step3, message=FALSE, warning=FALSE}
```

---

# Part 2:

## Step 1: Exploring and Preparing the Data

```{r part2Step1, message=FALSE, warning=FALSE}
```

## Step 2: Training a Model on the Data

```{r part2Step2, message=FALSE, warning=FALSE}
```

## Step 3: Evaluating Model Performance

```{r part2Step3, message=FALSE, warning=FALSE}
```

# ONLINE NEWS POPULARITY
------------------------

# Part 3:

## Step 1: Exploring and Preparing the Data

```{r part3Step1, message=FALSE, warning=FALSE}
```

## Step 2: Training a Model on the Data

```{r part3Step2, message=FALSE, warning=FALSE}
```

## Step 3: Evaluating Model Performance

```{r part3Step3, message=FALSE, warning=FALSE}
```

We randomize the whole dataset before splitting it into train and test data.  It makes no sense to randomize it after splitting the data.
In part 2, we choose to remove one highly correlated variable, and the accuracy is slightly improved compared to the first model.
In the last part, when we implement the method on News Popularity data, it gives a perfect prediction of 100% accuracy, which is much better than the result we got in lab 1.
